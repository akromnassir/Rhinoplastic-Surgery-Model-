{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "history_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zV9i-EJJISzf"
      },
      "source": [
        "pip install tensorflow-addons"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfIk2es3hJEd"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "import os\n",
        "import pathlib\n",
        "import time\n",
        "import datetime\n",
        "import math\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython import display\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYn4MdZnKCey"
      },
      "source": [
        "## Load the dataset\n",
        "\n",
        "Load your dataset. In Colab you can select other datasets from the drop-down menu. Note that some of the other datasets are significantly larger (`edges2handbags` is 8GB)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Iexs3ZMgXbK"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lMkRUesgb9p"
      },
      "source": [
        "import numpy as np\n",
        "before_1 = np.load(\"/content/drive/My Drive/RhinoplasticPaper/Data/Datasets/ds_before_8.npy\")\n",
        "after_1 = np.load(\"/content/drive/My Drive/RhinoplasticPaper/Data/Datasets/ds_after_8.npy\")\n",
        "before_2 = np.load(\"/content/drive/My Drive/RhinoplasticPaper/Data/Datasets/ds_before_fem.npy\")\n",
        "after_2 = np.load(\"/content/drive/My Drive/RhinoplasticPaper/Data/Datasets/ds_after_fem.npy\")\n",
        "\n",
        "before_1 = np.concatenate((before_1, np.zeros((before_1.shape[0], before_1.shape[1], before_1.shape[2],1))), axis = 3)\n",
        "before_2 = np.concatenate((before_2, np.zeros((before_2.shape[0], before_2.shape[1], before_2.shape[2],1))), axis = 3)\n",
        "after_1 = np.concatenate((after_1, np.zeros((after_1.shape[0], after_1.shape[1], after_1.shape[2],1))), axis = 3)\n",
        "after_2 = np.concatenate((after_2, np.zeros((after_2.shape[0], after_2.shape[1], after_2.shape[2],1))), axis = 3)\n",
        "\n",
        "train_before = np.concatenate((before_1[50:1420, ...], before_2[0:843, ...]), axis = 0)\n",
        "train_after = np.concatenate((after_1[50:1420, ...], after_2[0:843, ...]), axis = 0)\n",
        "test_before = np.concatenate((before_1[:50, ...], before_2[843:, ...]), axis = 0)\n",
        "test_after = np.concatenate((after_1[:50, ...], after_2[843:, ...]), axis = 0)\n",
        "\n",
        "train_df = np.concatenate((train_before, train_after), axis = 2)\n",
        "test_df = np.concatenate((test_before, test_after), axis = 2)\n",
        "\n",
        "train_df = tf.data.Dataset.from_tensor_slices((train_df))\n",
        "test_df = tf.data.Dataset.from_tensor_slices((test_df))\n",
        "\n",
        "print(len(train_df))\n",
        "print(len(test_df))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aO9ZAGH5K3SY"
      },
      "source": [
        "def load(image):\n",
        "  w = tf.shape(image)[1]\n",
        "  w = w // 2\n",
        "  input_image = image[:, :w, :]\n",
        "  real_image = image[:, w:, :]\n",
        "\n",
        "  # Convert both images to float32 tensors\n",
        "  input_image = tf.cast(input_image, tf.float32)\n",
        "  real_image = tf.cast(real_image, tf.float32)\n",
        "\n",
        "  return input_image, real_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5ByHTlfE06P"
      },
      "source": [
        "Plot a sample of the input (architecture label image) and real (building facade photo) images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CbTEt448b4R"
      },
      "source": [
        "BUFFER_SIZE = 9999\n",
        "# The batch size of 1 produced better results in the original pix2pix experiment\n",
        "BATCH_SIZE = 1\n",
        "IMG_WIDTH = 128\n",
        "IMG_HEIGHT = 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwwYQpu9FzDu"
      },
      "source": [
        "def resize(input_image, real_image, height, width):\n",
        "  input_image = tf.image.resize(input_image, [height, width],\n",
        "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "  real_image = tf.image.resize(real_image, [height, width],\n",
        "                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "\n",
        "  return input_image, real_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yn3IwqhiIszt"
      },
      "source": [
        "def random_crop(input_image, real_image):\n",
        "  stacked_image = tf.stack([input_image, real_image], axis=0)\n",
        "  cropped_image = tf.image.random_crop(\n",
        "      stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n",
        "\n",
        "  return cropped_image[0], cropped_image[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXLLV2PpGdbE"
      },
      "source": [
        "def random_saturation(input_image, real_image):\n",
        "  stacked_image = tf.stack([input_image, real_image], axis=0)\n",
        "  image = tf.image.random_saturation(\n",
        "      stacked_image, 0.6, 1.6)\n",
        "\n",
        "  return image[0], image[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gD29qToDG85L"
      },
      "source": [
        "def random_hue(input_image, real_image):\n",
        "  stacked_image = tf.stack([input_image, real_image], axis=0)\n",
        "  image = tf.image.random_hue(\n",
        "      stacked_image, 0.075)\n",
        "\n",
        "  return image[0], image[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IN2TVUJuMJhS"
      },
      "source": [
        "def random_contrast(input_image, real_image):\n",
        "  stacked_image = tf.stack([input_image, real_image], axis=0)\n",
        "  image = tf.image.random_contrast(\n",
        "      stacked_image, 0.8, 1.2)\n",
        "\n",
        "  return image[0], image[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muhR2cgbLKWW"
      },
      "source": [
        "# Normalizing the images to [-1, 1]\n",
        "def normalize(input_image, real_image):\n",
        "  input_image = (input_image / 127.5) - 1\n",
        "  real_image = (real_image / 127.5) - 1\n",
        "\n",
        "  return input_image, real_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVQOjcPVLrUc"
      },
      "source": [
        "@tf.function()\n",
        "def random_jitter(input_image, real_image, last_layer, train):\n",
        "  if train:\n",
        "    rs_f1 = 286 #tf.random.uniform(shape=(), minval=256, maxval=286, dtype=tf.int32)\n",
        "    rs_f2 = 143 #tf.random.uniform(shape=(), minval=128, maxval=138, dtype=tf.int32)\n",
        "    input_image, real_image = resize(input_image, real_image, rs_f1, rs_f2)\n",
        "    input_image, real_image = random_saturation(input_image, real_image)\n",
        "    input_image, real_image = random_contrast(input_image, real_image)\n",
        "\n",
        "    rn = tf.random.uniform(()) * 15\n",
        "    noise_1 = tf.random.normal(shape=tf.shape(input_image), mean=0.0, stddev=rn, dtype=tf.float32)\n",
        "    input_image = tf.add(input_image, noise_1)\n",
        "    input_image, real_image = random_crop(input_image, real_image)\n",
        "    input_image = tf.clip_by_value(input_image, 0 , 255)\n",
        "\n",
        "  if tf.random.uniform(()) > 0.5:\n",
        "    # Random mirroring\n",
        "    input_image = tf.image.flip_left_right(input_image)\n",
        "    real_image = tf.image.flip_left_right(real_image)\n",
        "    ones = tf.ones_like(input_image)\n",
        "  else:\n",
        "    ones = tf.zeros_like(input_image)\n",
        "  ones = ones[..., -1]\n",
        "  last_layer_1 = last_layer #tf.concat([last_layer, last_layer], axis = -2)\n",
        "  last_layer_2 = tf.expand_dims(ones, axis = -1)\n",
        "\n",
        "  return input_image, real_image, last_layer_1, last_layer_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfAQbzy799UV"
      },
      "source": [
        "You can inspect some of the preprocessed output:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyaP4hLJ8b4W"
      },
      "source": [
        "def load_image_train(image):\n",
        "  input_image, real_image = load(image)\n",
        "  last_layer_input_image = input_image[...,3]\n",
        "  last_layer_input_image = tf.expand_dims(last_layer_input_image, axis = -1)\n",
        "  input_image = input_image[...,:3]\n",
        "  real_image = real_image[...,:3]\n",
        "  input_image, real_image, last_layer_1, last_layer_2 = random_jitter(input_image, real_image, last_layer_input_image, True)\n",
        "  input_image, real_image = normalize(input_image, real_image)\n",
        "  input_image = tf.concat([input_image, last_layer_1, last_layer_2], axis = -1)\n",
        "\n",
        "  return input_image, real_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VB3Z6D_zKSru"
      },
      "source": [
        "def load_image_test(image):\n",
        "  input_image, real_image = load(image)\n",
        "\n",
        "  last_layer_input_image = input_image[...,3]\n",
        "  last_layer_input_image = tf.expand_dims(last_layer_input_image, axis = -1)\n",
        "  input_image = input_image[...,:3]\n",
        "  real_image = real_image[...,:3]\n",
        "  input_image, real_image = resize(input_image, real_image,\n",
        "                                   IMG_HEIGHT, IMG_WIDTH)\n",
        "  input_image, real_image, last_layer_1, last_layer_2 = random_jitter(input_image, real_image, last_layer_input_image, False)\n",
        "  input_image, real_image = normalize(input_image, real_image)\n",
        "  input_image = tf.concat([input_image, last_layer_1, last_layer_2], axis = -1)\n",
        "\n",
        "  return input_image, real_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIGN6ouoQxt3"
      },
      "source": [
        "## Build an input pipeline with `tf.data`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQHmYSmk8b4b"
      },
      "source": [
        "train_dataset = train_df\n",
        "train_dataset = train_dataset.map(load_image_train,\n",
        "                                  num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
        "\n",
        "plt.figure(figsize=(24, 24))\n",
        "it = iter(train_dataset)\n",
        "for ii in range(0, 32, 2):\n",
        "  im = next(it)\n",
        "  plt.subplot(8, 8, ii + 1)\n",
        "  plt.imshow((im[0][0][...,:3]+1)/2)\n",
        "  plt.subplot(8, 8, ii + 2)\n",
        "  plt.imshow((im[1][0]+1)/2)\n",
        "  plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MS9J0yA58b4g"
      },
      "source": [
        "try:\n",
        "  test_dataset = test_df\n",
        "except tf.errors.InvalidArgumentError:\n",
        "  test_dataset = test_df\n",
        "test_dataset = test_dataset.map(load_image_test)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqqvWxlw8b4l"
      },
      "source": [
        "OUTPUT_CHANNELS = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3R09ATE_SH9P"
      },
      "source": [
        "def downsample(filters, size, apply_batchnorm=True, strides = (2,2)):\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  result = tf.keras.Sequential()\n",
        "  result.add(\n",
        "      tf.keras.layers.Conv2D(filters, size, strides=strides, padding='same',\n",
        "                             kernel_initializer=initializer, use_bias=False))\n",
        "\n",
        "  if apply_batchnorm:\n",
        "    result.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "  result.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6_uCZCppTh7"
      },
      "source": [
        "down_model = downsample(3, 4)\n",
        "down_result = down_model(tf.expand_dims(im[0][0], 0))\n",
        "print (down_result.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFI_Pa52tjLl"
      },
      "source": [
        "Define the upsampler (decoder):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhgDsHClSQzP"
      },
      "source": [
        "def upsample(filters, size, apply_dropout=False, strides = 2):\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  result = tf.keras.Sequential()\n",
        "  result.add(\n",
        "    tf.keras.layers.Conv2DTranspose(filters, size, strides=strides,\n",
        "                                    padding='same',\n",
        "                                    kernel_initializer=initializer,\n",
        "                                    use_bias=False))\n",
        "\n",
        "  result.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "  if apply_dropout:\n",
        "      result.add(tf.keras.layers.Dropout(0.5))\n",
        "\n",
        "  result.add(tf.keras.layers.ReLU())\n",
        "\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mz-ahSdsq0Oc"
      },
      "source": [
        "up_model = upsample(3, 4)\n",
        "up_result = up_model(down_result)\n",
        "print (up_result.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueEJyRVrtZ-p"
      },
      "source": [
        "Define the generator with the downsampler and the upsampler:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFPI4Nu-8b4q"
      },
      "source": [
        "def Generator():\n",
        "  inputs = tf.keras.layers.Input(shape=[IMG_HEIGHT, IMG_WIDTH, 5])\n",
        "\n",
        "  #norm = tf.norm(inputs, ord=1, axis=3, keepdims=True)\n",
        "\n",
        "  down_stack = [\n",
        "    downsample(64, 4, apply_batchnorm=False),  # (batch_size, 128, 128, 64)\n",
        "    downsample(128, 4),  # (batch_size, 64, 64, 128)\n",
        "    downsample(256, 4),  # (batch_size, 32, 32, 256)\n",
        "    downsample(512, 4),  # (batch_size, 16, 16, 512)\n",
        "    downsample(512, 4),  # (batch_size, 8, 8, 512)\n",
        "    downsample(512, 4),  # (batch_size, 4, 4, 512)\n",
        "    downsample(512, 4),  # (batch_size, 2, 2, 512)\n",
        "    downsample(512, 4, strides = (2,1)),  # (batch_size, 1, 1, 512)\n",
        "  ]\n",
        "\n",
        "  up_stack = [\n",
        "    upsample(1024, 4, apply_dropout=True, strides = (2,1)),  # (batch_size, 2, 2, 1024)\n",
        "    upsample(1024, 4, apply_dropout=True),  # (batch_size, 4, 4, 1024)\n",
        "    upsample(1024, 4, apply_dropout=True),  # (batch_size, 8, 8, 1024)\n",
        "    upsample(1024, 4),  # (batch_size, 16, 16, 1024)\n",
        "    upsample(512, 4),  # (batch_size, 32, 32, 512)\n",
        "    upsample(256, 4),  # (batch_size, 64, 64, 256)\n",
        "    upsample(128, 4),  # (batch_size, 128, 128, 128)\n",
        "  ]\n",
        "\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "  last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n",
        "                                         strides=2,\n",
        "                                         padding='same',\n",
        "                                         kernel_initializer=initializer,\n",
        "                                         activation='tanh')  # (batch_size, 256, 256, 3)\n",
        "\n",
        "  x = inputs\n",
        "\n",
        "  # Downsampling through the model\n",
        "  skips = []\n",
        "  for down in down_stack:\n",
        "    x = down(x)\n",
        "    skips.append(x)\n",
        "\n",
        "  skips = reversed(skips[:-1])\n",
        "\n",
        "  # Upsampling and establishing the skip connections\n",
        "  for up, skip in zip(up_stack, skips):\n",
        "    x = up(x)\n",
        "    x = tf.keras.layers.Concatenate()([x, skip])\n",
        "\n",
        "  x = last(x)\n",
        "\n",
        "  return tf.keras.Model(inputs=inputs, outputs=x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4PKwrcQFYvF"
      },
      "source": [
        "Visualize the generator model architecture:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIbRPFzjmV85"
      },
      "source": [
        "generator = Generator()\n",
        "tf.keras.utils.plot_model(generator, show_shapes=True, dpi=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8kbgTK8FcPo"
      },
      "source": [
        "Test the generator:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1N1_obwtdQH"
      },
      "source": [
        "gen_output = generator(im[0][0][tf.newaxis, ...], training=False)\n",
        "plt.imshow(gen_output[0, ...])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpDPEQXIAiQO"
      },
      "source": [
        "### Define the generator loss\n",
        "\n",
        "GANs learn a loss that adapts to the data, while cGANs learn a structured loss that penalizes a possible structure that differs from the network output and the target image, as described in the [pix2pix paper](https://arxiv.org/abs/1611.07004).\n",
        "\n",
        "- The generator loss is a sigmoid cross-entropy loss of the generated images and an **array of ones**.\n",
        "- The pix2pix paper also mentions the L1 loss, which is a MAE (mean absolute error) between the generated image and the target image.\n",
        "- This allows the generated image to become structurally similar to the target image.\n",
        "- The formula to calculate the total generator loss is `gan_loss + LAMBDA * l1_loss`, where `LAMBDA = 100`. This value was decided by the authors of the paper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyhxTuvJyIHV"
      },
      "source": [
        "LAMBDA = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1Xbz5OaLj5C"
      },
      "source": [
        "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90BIcCKcDMxz"
      },
      "source": [
        "def generator_loss(disc_generated_output, gen_output, target):\n",
        "  gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
        "\n",
        "  # Mean absolute error\n",
        "  l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
        "\n",
        "  total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
        "\n",
        "  return total_gen_loss, gan_loss, l1_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSZbDgESHIV6"
      },
      "source": [
        "The training procedure for the generator is as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlB-XMY5Awj9"
      },
      "source": [
        "![Generator Update Image](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/images/gen.png?raw=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTKZfoaoEF22"
      },
      "source": [
        "## Build the discriminator\n",
        "\n",
        "The discriminator in the pix2pix cGAN is a convolutional PatchGAN classifier—it tries to classify if each image _patch_ is real or not real, as described in the [pix2pix paper](https://arxiv.org/abs/1611.07004).\n",
        "\n",
        "- Each block in the discriminator is: Convolution -> Batch normalization -> Leaky ReLU.\n",
        "- The shape of the output after the last layer is `(batch_size, 30, 30, 1)`.\n",
        "- Each `30 x 30` image patch of the output classifies a `70 x 70` portion of the input image.\n",
        "- The discriminator receives 2 inputs:\n",
        "    - The input image and the target image, which it should classify as real.\n",
        "    - The input image and the generated image (the output of the generator), which it should classify as fake.\n",
        "    - Use `tf.concat([inp, tar], axis=-1)` to concatenate these 2 inputs together."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIuTeGL5v45m"
      },
      "source": [
        "Let's define the discriminator:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ll6aNeQx8b4v"
      },
      "source": [
        "def Discriminator():\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  inp = tf.keras.layers.Input(shape=[IMG_HEIGHT, IMG_WIDTH, 5], name='input_image')\n",
        "  tar = tf.keras.layers.Input(shape=[IMG_HEIGHT, IMG_WIDTH, 3], name='target_image')\n",
        "\n",
        "  #norm, _ = tf.linalg.normalize(inp, ord=1, axis=3)\n",
        "\n",
        "  x = tf.keras.layers.concatenate([inp, tar])  # (batch_size, 256, 256, channels*2)\n",
        "\n",
        "  down1 = downsample(64, 4, False)(x)  # (batch_size, 128, 128, 64)\n",
        "  down2 = downsample(128, 4)(down1)  # (batch_size, 64, 64, 128)\n",
        "  down3 = downsample(256, 4)(down2)  # (batch_size, 32, 32, 256)\n",
        "\n",
        "  zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3)  # (batch_size, 34, 34, 256)\n",
        "  conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n",
        "                                kernel_initializer=initializer,\n",
        "                                use_bias=False)(zero_pad1)  # (batch_size, 31, 31, 512)\n",
        "\n",
        "  batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
        "\n",
        "  leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
        "\n",
        "  zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)  # (batch_size, 33, 33, 512)\n",
        "\n",
        "  last = tf.keras.layers.Conv2D(1, 4, strides=1,\n",
        "                                kernel_initializer=initializer)(zero_pad2)  # (batch_size, 30, 30, 1)\n",
        "\n",
        "  return tf.keras.Model(inputs=[inp, tar], outputs=last)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdV9yAbBHNkg"
      },
      "source": [
        "Visualize the discriminator model architecture:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHoUui4om-Ev"
      },
      "source": [
        "discriminator = Discriminator()\n",
        "tf.keras.utils.plot_model(discriminator, show_shapes=True, dpi=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ps7nIHigHYc7"
      },
      "source": [
        "Test the discriminator:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDkA05NE6QMs"
      },
      "source": [
        "target = im[0][0][tf.newaxis, ...]\n",
        "disc_out = discriminator([im[0][0][tf.newaxis, ...], gen_output], training=False)\n",
        "plt.imshow(disc_out[0, ..., -1], vmin=-20, vmax=20, cmap='RdBu_r')\n",
        "plt.colorbar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOqg1dhUAWoD"
      },
      "source": [
        "### Define the discriminator loss\n",
        "\n",
        "- The `discriminator_loss` function takes 2 inputs: **real images** and **generated images**.\n",
        "- `real_loss` is a sigmoid cross-entropy loss of the **real images** and an **array of ones(since these are the real images)**.\n",
        "- `generated_loss` is a sigmoid cross-entropy loss of the **generated images** and an **array of zeros (since these are the fake images)**.\n",
        "- The `total_loss` is the sum of `real_loss` and `generated_loss`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkMNfBWlT-PV"
      },
      "source": [
        "def discriminator_loss(disc_real_output, disc_generated_output):\n",
        "  real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
        "\n",
        "  generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
        "\n",
        "  total_disc_loss = real_loss + generated_loss\n",
        "\n",
        "  return total_disc_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbHFNexF0x6O"
      },
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJnftd5sQsv6"
      },
      "source": [
        "checkpoint_dir = '/content/drive/My Drive/RhinoplasticPaper/Models/pix2pix/model_12'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4t4x69adQ5xb"
      },
      "source": [
        "# Restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmdVsmvhPxyy"
      },
      "source": [
        "import cv2\n",
        "def generate_images(model, test_input, tar):\n",
        "  prediction = model(test_input, training=True)\n",
        "  plt.figure(figsize=(15, 15))\n",
        "\n",
        "  display_list = [test_input[0][...,:3], tar[0][...,:3], prediction[0][...,:3]]\n",
        "  title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
        "\n",
        "  for i in range(3):\n",
        "    plt.subplot(1, 3, i+1)\n",
        "    plt.title(title[i])\n",
        "    # Getting the pixel values in the [0, 1] range to plot.\n",
        "    im = cv2.resize(np.float32(display_list[i]), dsize=(128, 256))\n",
        "    plt.imshow(im * 0.5 + 0.5)\n",
        "    plt.axis('off')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gipsSEoZIG1a"
      },
      "source": [
        "Test the function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Fc4NzT-DgEx"
      },
      "source": [
        "for example_input, example_target in test_dataset.take(1):\n",
        "  generate_images(generator, example_input, example_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLKOG55MErD0"
      },
      "source": [
        "## Training\n",
        "\n",
        "- For each example input generates an output.\n",
        "- The discriminator receives the `input_image` and the generated image as the first input. The second input is the `input_image` and the `target_image`.\n",
        "- Next, calculate the generator and the discriminator loss.\n",
        "- Then, calculate the gradients of loss with respect to both the generator and the discriminator variables(inputs) and apply those to the optimizer.\n",
        "- Finally, log the losses to TensorBoard."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNNMDBNH12q-"
      },
      "source": [
        "log_dir=\"/content/drive/MyDrive/pix2pix/logs/\"\n",
        "\n",
        "summary_writer = tf.summary.create_file_writer(\n",
        "  log_dir + \"fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBKUV2sKXDbY"
      },
      "source": [
        "@tf.function\n",
        "def train_step(input_image, target, step):\n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "    gen_output = generator(input_image, training=True)\n",
        "    disc_real_output = discriminator([input_image, target], training=True)\n",
        "    disc_generated_output = discriminator([input_image, gen_output], training=True)\n",
        "\n",
        "    gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)\n",
        "    disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
        "\n",
        "  generator_gradients = gen_tape.gradient(gen_total_loss,\n",
        "                                          generator.trainable_variables)\n",
        "  discriminator_gradients = disc_tape.gradient(disc_loss,\n",
        "                                               discriminator.trainable_variables)\n",
        "\n",
        "  generator_optimizer.apply_gradients(zip(generator_gradients,\n",
        "                                          generator.trainable_variables))\n",
        "  discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n",
        "                                              discriminator.trainable_variables))\n",
        "\n",
        "  with summary_writer.as_default():\n",
        "    tf.summary.scalar('gen_total_loss', gen_total_loss, step=step//1000)\n",
        "    tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=step//1000)\n",
        "    tf.summary.scalar('gen_l1_loss', gen_l1_loss, step=step//1000)\n",
        "    tf.summary.scalar('disc_loss', disc_loss, step=step//1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hx7s-vBHFKdh"
      },
      "source": [
        "The actual training loop. Since this tutorial can run of more than one dataset, and the datasets vary greatly in size the training loop is setup to work in steps instead of epochs.\n",
        "\n",
        "- Iterates over the number of steps.\n",
        "- Every 10 steps print a dot (`.`).\n",
        "- Every 1k steps: clear the display and run `generate_images` to show the progress.\n",
        "- Every 5k steps: save a checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFyPlBWv1B5j"
      },
      "source": [
        "def fit(train_ds, test_ds, steps):\n",
        "  example_input, example_target = next(iter(test_ds.take(1)))\n",
        "  start = time.time()\n",
        "\n",
        "  for step, (input_image, target) in train_ds.repeat().take(steps).enumerate():\n",
        "    if (step) % 1000 == 0:\n",
        "      display.clear_output(wait=True)\n",
        "\n",
        "      if step != 0:\n",
        "        print(f'Time taken for 1000 steps: {time.time()-start:.2f} sec\\n')\n",
        "\n",
        "      start = time.time()\n",
        "\n",
        "      for example_input, example_target in test_dataset.take(1):\n",
        "        generate_images(generator, example_input, example_target)\n",
        "      for example_input, example_target in train_dataset.take(1):\n",
        "        generate_images(generator, example_input, example_target)\n",
        "      print(f\"Step: {step//1000}k\")\n",
        "\n",
        "    train_step(input_image, target, step)\n",
        "\n",
        "    # Training step\n",
        "    if (step+1) % 10 == 0:\n",
        "      print('.', end='', flush=True)\n",
        "\n",
        "\n",
        "    # Save (checkpoint) the model every 5k steps\n",
        "    if (step + 1) % 5000 == 0:\n",
        "      checkpoint.save(file_prefix=checkpoint_prefix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wozqyTh2wmCu"
      },
      "source": [
        "This training loop saves logs that you can view in TensorBoard to monitor the training progress.\n",
        "\n",
        "If you work on a local machine, you would launch a separate TensorBoard process. When working in a notebook, launch the viewer before starting the training to monitor with TensorBoard.\n",
        "\n",
        "To launch the viewer paste the following into a code-cell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ot22ujrlLhOd"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {log_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pe0-8Bzg22ox"
      },
      "source": [
        "Finally, run the training loop:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1zZmKmvOH85"
      },
      "source": [
        "fit(train_dataset, test_dataset, steps=500000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeq9sByu86-B"
      },
      "source": [
        "If you want to share the TensorBoard results _publicly_, you can upload the logs to [TensorBoard.dev](https://tensorboard.dev/) by copying the following into a code-cell.\n",
        "\n",
        "Note: This requires a Google account.\n",
        "\n",
        "```\n",
        "!tensorboard dev upload --logdir {log_dir}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-kT7WHRKz-E"
      },
      "source": [
        "Caution: This command does not terminate. It's designed to continuously upload the results of long-running experiments. Once your data is uploaded you need to stop it using the \"interrupt execution\" option in your notebook tool."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lGhS_LfwQoL"
      },
      "source": [
        "You can view the [results of a previous run](https://tensorboard.dev/experiment/lZ0C6FONROaUMfjYkVyJqw) of this notebook on [TensorBoard.dev](https://tensorboard.dev/).\n",
        "\n",
        "TensorBoard.dev is a managed experience for hosting, tracking, and sharing ML experiments with everyone.\n",
        "\n",
        "It can also included inline using an `<iframe>`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IS4c93guQ8E"
      },
      "source": [
        "display.IFrame(\n",
        "    src=\"https://tensorboard.dev/experiment/lZ0C6FONROaUMfjYkVyJqw\",\n",
        "    width=\"100%\",\n",
        "    height=\"1000px\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMTm4peo3cem"
      },
      "source": [
        "Interpreting the logs is more subtle when training a GAN (or a cGAN like pix2pix) compared to a simple classification or regression model. Things to look for:\n",
        "\n",
        "- Check that neither the generator nor the discriminator model has \"won\". If either the `gen_gan_loss` or the `disc_loss` gets very low, it's an indicator that this model is dominating the other, and you are not successfully training the combined model.\n",
        "- The value `log(2) = 0.69` is a good reference point for these losses, as it indicates a perplexity of 2 - the discriminator is, on average, equally uncertain about the two options.\n",
        "- For the `disc_loss`, a value below `0.69` means the discriminator is doing better than random on the combined set of real and generated images.\n",
        "- For the `gen_gan_loss`, a value below `0.69` means the generator is doing better than random at fooling the discriminator.\n",
        "- As training progresses, the `gen_l1_loss` should go down."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kz80bY3aQ1VZ"
      },
      "source": [
        "## Restore the latest checkpoint and test the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RGysMU_BZhx"
      },
      "source": [
        "## Generate some images using the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUgSnmy2nqSP"
      },
      "source": [
        "# Run the trained model on a few examples from the test set\n",
        "for inp, tar in test_dataset.take(29):\n",
        "  generate_images(generator, inp, tar)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
